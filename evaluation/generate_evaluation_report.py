

import sys
import os
import glob
from datetime import datetime

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


from evaluation.utils.file_utils import read_jsonl_file

INPUT_DIR = "evaluation/data/evaluation/"
OUTPUT_DIR = "evaluation/data/report/"
OUTPUT_FILE_NAME = "evaluation_report.md"


def generate_evaluation_report():
    
    # à¸„à¹‰à¸™à¸«à¸²à¹„à¸Ÿà¸¥à¹Œà¸œà¸¥à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸¥à¹ˆà¸²à¸ªà¸¸à¸”
    evaluation_files = glob.glob(os.path.join(INPUT_DIR, "*_llm_judge_evaluation_results.jsonl"))
    
    if not evaluation_files:
        print(f"No evaluation files found in {INPUT_DIR}")
        return
    
    # à¸”à¸¶à¸‡à¹„à¸Ÿà¸¥à¹Œà¸¥à¹ˆà¸²à¸ªà¸¸à¸” (à¸•à¸²à¸¡à¹€à¸§à¸¥à¸²à¸—à¸µà¹ˆà¹à¸à¹‰à¹„à¸‚)
    latest_file = max(evaluation_files, key=os.path.getmtime)
    print(f"Using evaluation file: {latest_file}")
    
    # à¸­à¹ˆà¸²à¸™à¸œà¸¥à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™
    evaluation_results = read_jsonl_file(latest_file)
    print(f"Loaded {len(evaluation_results)} evaluation results")
    
    # à¹à¸¢à¸à¸Šà¸·à¹ˆà¸­à¹„à¸Ÿà¸¥à¹Œà¸ªà¸³à¸«à¸£à¸±à¸šà¸Šà¸·à¹ˆà¸­à¸£à¸²à¸¢à¸‡à¸²à¸™à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸‹à¹‰à¸³
    input_filename = os.path.basename(latest_file)
    timestamp_part = input_filename.split('_')[0]  # à¹à¸¢à¸ timestamp à¸ˆà¸²à¸à¸Šà¸·à¹ˆà¸­à¹„à¸Ÿà¸¥à¹Œ
    unique_report_name = f"{timestamp_part}_evaluation_report.md"
    
    # à¸„à¸³à¸™à¸§à¸“à¸ªà¸–à¸´à¸•à¸´à¸ªà¸£à¸¸à¸›
    scores = [result['score'] for result in evaluation_results]
    avg_score = sum(scores) / len(scores) if scores else 0
    score_distribution = {}
    for score in set(scores):
        score_distribution[score] = scores.count(score)
    
    # à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸”à¹€à¸£à¹‡à¸à¸—à¸­à¸£à¸µà¹€à¸­à¸²à¸•à¹Œà¸žà¸¸à¸•à¸«à¸²à¸à¹„à¸¡à¹ˆà¸¡à¸µà¸­à¸¢à¸¹à¹ˆ
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    
    # à¸ªà¸£à¹‰à¸²à¸‡à¸£à¸²à¸¢à¸‡à¸²à¸™ Markdown
    report_content = generate_markdown_report(evaluation_results, avg_score, score_distribution)
    
    # à¹€à¸‚à¸µà¸¢à¸™à¸£à¸²à¸¢à¸‡à¸²à¸™à¸¥à¸‡à¹„à¸Ÿà¸¥à¹Œ
    output_path = os.path.join(OUTPUT_DIR, unique_report_name)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"Evaluation report generated: {output_path}")
    print(f"Summary: {len(evaluation_results)} questions evaluated, average score: {avg_score:.2f}/5")
    
    return output_path


def generate_markdown_report(evaluation_results, avg_score, score_distribution):
    """à¸ªà¸£à¹‰à¸²à¸‡à¸£à¸²à¸¢à¸‡à¸²à¸™ Markdown à¸—à¸µà¹ˆà¹€à¸£à¸µà¸¢à¸šà¸‡à¹ˆà¸²à¸¢à¹à¸¥à¸°à¸Šà¸±à¸”à¹€à¸ˆà¸™à¸ˆà¸²à¸à¸œà¸¥à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™"""
    
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    total_questions = len(evaluation_results)
    
    report = f"""# RAG System Evaluation Report

Generated: {current_time}

## Summary
- **Questions Tested:** {total_questions}
- **Average Score:** {avg_score:.1f}/5

## Results
"""
    
    # Simple score breakdown
    for score in sorted(score_distribution.keys(), reverse=True):
        count = score_distribution[score]
        report += f"- **{score}/5**: {count} questions\n"
    
    report += f"""
## Question Details

"""
    
    # à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¸„à¸³à¸–à¸²à¸¡à¸—à¸µà¸¥à¸°à¸‚à¹‰à¸­
    for i, result in enumerate(evaluation_results, 1):
        # à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸ªà¸µà¸•à¸²à¸¡à¸„à¸°à¹à¸™à¸™
        score_emoji = {5: "ðŸŸ¢", 4: "ðŸŸ¡", 3: "ðŸŸ ", 2: "ðŸ”´", 1: "âš«"}
        emoji = score_emoji.get(result['score'], "âšª")
        
        report += f"""---
### {emoji} Q{i}: {result['question']}

**ðŸ“Š Score: `{result['score']}/5`**

**âœ… Expected Answer:**
> {result['answer']}

**ðŸ¤– LLM Answer:**
> {result['llmAnswer']}

**âš–ï¸ Judge Reasoning:**
> {result['reason']}

"""
    
    # à¸ªà¸£à¸¸à¸›à¹€à¸£à¸µà¸¢à¸šà¸‡à¹ˆà¸²à¸¢
    if avg_score >= 4:
        performance = "Excellent"
    elif avg_score >= 3:
        performance = "Good"
    elif avg_score >= 2:
        performance = "Fair"
    else:
        performance = "Needs Improvement"
    
    report += f"""## Conclusion
The RAG system performance is **{performance}** with an average score of {avg_score:.1f}/5.
"""
    
    return report
    








if __name__ == "__main__":
    generate_evaluation_report()
    